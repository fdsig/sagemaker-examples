{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Amazon Sagemaker Distributed Model Parallel to Launch a BERT Training Job with Model Parallelization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sagemaker distributed model parallel (SMP) is a model parallelism library for training large deep learning models that were previously difficult to train due to GPU memory limitations. SMP automatically and efficiently splits a model across multiple GPUs and instances and coordinates model training, allowing you to increase prediction accuracy by creating larger models with more parameters.\n",
    "\n",
    "Use this notebook to configure SMP to train a model using PyTorch (version 1.6.0) and the [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#train-a-model-with-the-sagemaker-python-sdk).\n",
    "\n",
    "In this notebook, you will use a BERT example training script with SMP.\n",
    "The example script is based on [Nvidia Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT) and requires you to download the datasets and upload them to Amazon Simple Storage Service (Amazon S3) as explained in the instructions below. This is a large dataset, and so depending on your connection speed, this process can take hours to complete. \n",
    "\n",
    "This notebook depends on the following files. You can find all files in the [bert directory](https://github.com/aws/amazon-sagemaker-examples/tree/master/training/distributed_training/pytorch/model_parallel/bert) in the model parllel section of the Amazon SageMaker Examples notebooks repo.\n",
    "\n",
    "* `bert_example/sagemaker_smp_pretrain.py`: This is an entrypoint script that is passed to the Pytorch estimator in the notebook instructions. This script is responsible for end to end training of the BERT model with SMP. The script has additional comments at places where the SMP API is used.\n",
    "\n",
    "* `bert_example/modeling.py`: This contains the model definition for the BERT model.\n",
    "\n",
    "* `bert_example/bert_config.json`: This allows for additional configuration of the model and is used by `modeling.py`. Additional configuration includes dropout probabilities, pooler and encoder sizes, number of hidden layers in the encoder, size of the intermediate layers in the encoder etc.\n",
    "\n",
    "* `bert_example/schedulers.py`: contains definitions for learning rate schedulers used in end to end training of the BERT model (`bert_example/sagemaker_smp_pretrain.py`).\n",
    "\n",
    "* `bert_example/utils.py`: This contains different helper utility functions used in end to end training of the BERT model (`bert_example/sagemaker_smp_pretrain.py`).\n",
    "\n",
    "* `bert_example/file_utils.py`: Contains different file utility functions used in model definition (`bert_example/modeling.py`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to learn more about SMP and using SageMaker with Pytorch. \n",
    "\n",
    "* To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](http://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "* To learn more about using the SageMaker Python SDK with Pytorch, see [Using PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "* To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html).\n",
    "\n",
    "\n",
    "### Prerequisites \n",
    "\n",
    "1. You must create an S3 bucket to store the input data to be used for training. This bucket must must be located in the same AWS Region you use to launch your training job. This is the AWS Region you use to run this notebook. To learn how, see [Creating a bucket](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html) in the Amazon S3 documentation.\n",
    "\n",
    "2. You must download the dataset that you use for training from [Nvidia Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT) and upload it to the S3 bucket you created. To learn more about the datasets and scripts provided to preprocess and download it, see [Getting the data](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md#getting-the-data) in the Nvidia Deep Learning Examples repo README. You can also use the [Quick Start Guide](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md#quick-start-guide) to learn how to download the dataset. The repository consists of three datasets. Optionally, you can to use the `wiki_only` parameter to only download the Wikipedia dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade Sagemaker SDK to the latest version.\n",
    "NOTE: This step may require a kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.152.0)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.15.1-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.6.2)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.26.125)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.22.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.5.2)\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (5.4.1)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (65.6.3)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.21.1-py2.py3-none-any.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.7/201.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (2.28.1)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Using cached GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.125 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.125)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=93808d0298a3afb586e3554be2f6d78cc5caef161d693381e15e2b0efa460a39\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/44/1b/54/249c94316d4e1030e2d0683fba1d8ea06197de866f5a4de738\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, urllib3, smmap, setproctitle, docker-pycreds, sentry-sdk, gitdb, GitPython, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.8\n",
      "    Uninstalling urllib3-1.26.8:\n",
      "      Successfully uninstalled urllib3-1.26.8\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.18.0 requires jsonschema>=4.17.3, but you have jsonschema 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.21.1 setproctitle-1.3.2 smmap-5.0.0 urllib3-1.26.15 wandb-0.15.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "original_version = sagemaker.__version__\n",
    "%pip install --upgrade sagemaker wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the notebook instance. Get the AWS Region, SageMaker execution role Amazon Resource Name (ARN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role:arn:aws:iam::601636808299:role/support-sagemaker-frida\n",
      "AWS account:601636808299\n",
      "AWS region:us-east-2\n",
      "['/home/ec2-user/SageMaker/sagemaker-examples/training/distributed_training/pytorch/model_parallel/bert', '/home/ec2-user/anaconda3/envs/python3/lib/python310.zip', '/home/ec2-user/anaconda3/envs/python3/lib/python3.10', '/home/ec2-user/anaconda3/envs/python3/lib/python3.10/lib-dynload', '', '/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages']\n",
      "\n",
      "Default bucket for this session:  sagemaker-us-east-2-601636808299\n",
      "CPU times: user 698 ms, sys: 70.2 ms, total: 768 ms\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "import sys\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare/Identify your Training Data in Amazon S3\n",
    "\n",
    "If you don't already have the BERT dataset in an S3 bucket, please see the instructions in [Nvidia BERT Example](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md) to download the dataset and upload it to a s3 bucket. See the prerequisites at the beginning of this notebook for more information.\n",
    "\n",
    "Replace the instances of `None` below to set the S3 bucket and prefix of your preprocessed\n",
    "data. For example, if your training data is in s3://your-bucket/training, enter `'your-bucket'` for `s3_bucket` and `'training'` for `prefix`. Note that your output data will be stored in the same bucket, under the `output/` prefix.\n",
    "\n",
    "If you proceed with `None` values for both `s3_bucket` and `prefix`, then the program downloads some mock data from a public S3 bucket `sagemaker-sample-files` and uploads it\n",
    "to your default bucket. This is intended for CI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_bucket = None  # Replace None by your bucket\n",
    "prefix = None  # Replace None by the prefix of your data\n",
    "\n",
    "# For CI\n",
    "if s3_bucket is None:\n",
    "    # Donwload some mock data from a public bucket in us-east-1\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket_name = \"sagemaker-sample-files\"\n",
    "    # Phase 1 pretraining\n",
    "    prefix = \"datasets/binary/bert/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en_abstract\"\n",
    "\n",
    "    local_dir = \"/tmp/data\"\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    for obj in bucket.objects.filter(Prefix=prefix):\n",
    "        target = os.path.join(local_dir, obj.key)\n",
    "        if not os.path.exists(os.path.dirname(target)):\n",
    "            os.makedirs(os.path.dirname(target))\n",
    "        bucket.download_file(obj.key, target)\n",
    "\n",
    "    # upload to default bucket\n",
    "    mock_data = sagemaker_session.upload_data(\n",
    "        path=os.path.join(local_dir, prefix),\n",
    "        bucket=sagemaker_session.default_bucket(),\n",
    "        key_prefix=prefix,\n",
    "    )\n",
    "\n",
    "    data_channels = {\"train\": mock_data}\n",
    "else:\n",
    "\n",
    "    s3train = f\"s3://{s3_bucket}/{prefix}\"\n",
    "    train = sagemaker.session.TrainingInput(\n",
    "        s3train, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "    )\n",
    "    data_channels = {\"train\": train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-east-2-601636808299/datasets/binary/bert/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en_abstract'}\n"
     ]
    }
   ],
   "source": [
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your output data path. This is where model artifacts are stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your output data will be stored in: s3://sagemaker-us-east-2-601636808299/output/bert\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = f\"s3://{default_bucket}/output/bert\"\n",
    "print(f\"your output data will be stored in: s3://{default_bucket}/output/bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Training Job\n",
    "\n",
    "Next, you will use SageMaker Estimator API to define a SageMaker Training Job. You will use a [`PyTorchEstimator`](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html) to define the number and type of EC2 instances Amazon SageMaker uses for training, as well as the size of the volume attached to those instances. \n",
    "\n",
    "You must update the following:\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "\n",
    "See the following sub-sections for more details. \n",
    "\n",
    "### Update the Type and Number of EC2 Instances Used\n",
    "\n",
    "The instance type and number of instances you specify in `instance_type` and `instance_count` respectively will determine the number of GPUs Amazon SageMaker uses during training. Explicitly, `instance_type` will determine the number of GPUs on a single instance and that number will be multiplied by `instance_count`. \n",
    "\n",
    "You must specify values for `instance_type` and `instance_count` so that the total number of GPUs available for training is equal to `partitions` in `config` of `smp.init` in your training script. \n",
    "\n",
    "If you set ddp to `True`, you must ensure that the total number of GPUs available is divisible by `partitions`. The result of the division is inferred to be the number of model replicas to be used for Horovod (data parallelism degree). \n",
    "\n",
    "See [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/) for SageMaker supported instances and cost information. To look up GPUs for each instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that an ml.p3.2xlarge has the same number of GPUs as an p3.2xlarge.\n",
    "\n",
    "### Update your Volume Size\n",
    "\n",
    "The volume size you specify in `volume_size` must be larger than your input data size.\n",
    "\n",
    "### Set your parameters dictionary for SMP and set custom mpioptions\n",
    "\n",
    "With the parameters dictionary you can configure: the number of microbatches, number of partitions, whether to use data parallelism with ddp, the pipelining strategy, the placement strategy and other BERT specific hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpi_options = \"-verbose --mca orte_base_help_aggregate 0 \"\n",
    "smp_parameters = {\n",
    "    \"optimize\": \"speed\",\n",
    "    \"microbatches\": 12,\n",
    "    \"partitions\": 2,\n",
    "    \"ddp\": True,\n",
    "    \"pipeline\": \"interleaved\",\n",
    "    \"overlapping_allreduce\": True,\n",
    "    \"placement_strategy\": \"cluster\",\n",
    "    \"memory_weight\": 0.3,\n",
    "}\n",
    "timeout = 60 * 60\n",
    "metric_definitions = [{\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}]\n",
    "\n",
    "hyperparameters = {\n",
    "    \"input_dir\": \"/opt/ml/input/data/train\",\n",
    "    \"output_dir\": \"./checkpoints\",\n",
    "    \"config_file\": \"bert_config.json\",\n",
    "    \"bert_model\": \"bert-large-uncased\",\n",
    "    \"train_batch_size\": 48,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"max_predictions_per_seq\": 20,\n",
    "    \"max_steps\": 7038,\n",
    "    \"warmup_proportion\": 0.2843,\n",
    "    \"num_steps_per_checkpoint\": 200,\n",
    "    \"learning_rate\": 6e-3,\n",
    "    \"seed\": 12439,\n",
    "    \"steps_this_run\": 500,\n",
    "    \"allreduce_post_accumulation\": 1,\n",
    "    \"allreduce_post_accumulation_fp16\": 1,\n",
    "    \"do_train\": 1,\n",
    "    \"use_sequential\": 1,\n",
    "    \"skip_checkpoint\": 1,\n",
    "    \"smp\": 1,\n",
    "    \"apply_optimizer\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Pytorch Estimator with SMP enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    \"sagemaker_smp_pretrain.py\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.p3.8xlarge\",\n",
    "    volume_size=200,\n",
    "    instance_count=1,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    py_version=\"py36\",\n",
    "    framework_version=\"1.6.0\",\n",
    "    distribution={\n",
    "        \"smdistributed\": {\"modelparallel\": {\"enabled\": True, \"parameters\": smp_parameters}},\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": 4,\n",
    "            \"custom_mpi_options\": mpi_options,\n",
    "        },\n",
    "    },\n",
    "    source_dir=\"bert_example\",\n",
    "    output_path=s3_output_location,\n",
    "    max_run=timeout,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will use the estimator to launch the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-05-05-08-49-07-014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-05 08:49:07 Starting - Starting the training job......\n",
      "2023-05-05 08:49:51 Starting - Preparing the instances for training......\n",
      "2023-05-05 08:51:05 Downloading - Downloading input data\n",
      "2023-05-05 08:51:05 Training - Downloading the training image...............\n",
      "2023-05-05 08:53:21 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:00,797 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:00,844 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:00,847 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:01,047 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting wandb\u001b[0m\n",
      "\u001b[34m  Downloading wandb-0.15.1-py3-none-any.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.21.1-py2.py3-none-any.whl (201 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (3.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (5.8.0)\u001b[0m\n",
      "\u001b[34mCollecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8785 sha256=12dfdb83671529052aa4d04cbc6f462b6506f0a29d0f7546415d8c521e4c12ee\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/ea/90/e37d463fb3b03848bf715080595de62545266f53dd546b2497\u001b[0m\n",
      "\u001b[34mSuccessfully built pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: smmap, urllib3, gitdb, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, appdirs, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.18 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.21.1 setproctitle-1.2.3 smmap-5.0.0 urllib3-1.26.15 wandb-0.15.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:06,860 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:06,860 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:06,862 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:06,862 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:8'] process_per_hosts: 8 num_processes: 8\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:06,863 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:06,912 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.8xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-verbose --mca orte_base_help_aggregate 0 \",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"allreduce_post_accumulation\": 1,\n",
      "        \"allreduce_post_accumulation_fp16\": 1,\n",
      "        \"apply_optimizer\": 1,\n",
      "        \"bert_model\": \"bert-large-uncased\",\n",
      "        \"config_file\": \"bert_config.json\",\n",
      "        \"do_train\": 1,\n",
      "        \"input_dir\": \"/opt/ml/input/data/train\",\n",
      "        \"learning_rate\": 0.006,\n",
      "        \"max_predictions_per_seq\": 20,\n",
      "        \"max_seq_length\": 128,\n",
      "        \"max_steps\": 7038,\n",
      "        \"mp_parameters\": {\n",
      "            \"optimize\": \"speed\",\n",
      "            \"microbatches\": 12,\n",
      "            \"partitions\": 2,\n",
      "            \"ddp\": true,\n",
      "            \"pipeline\": \"interleaved\",\n",
      "            \"overlapping_allreduce\": true,\n",
      "            \"placement_strategy\": \"cluster\",\n",
      "            \"memory_weight\": 0.3\n",
      "        },\n",
      "        \"num_steps_per_checkpoint\": 200,\n",
      "        \"output_dir\": \"./checkpoints\",\n",
      "        \"seed\": 12439,\n",
      "        \"skip_checkpoint\": 1,\n",
      "        \"smp\": 1,\n",
      "        \"steps_this_run\": 500,\n",
      "        \"train_batch_size\": 48,\n",
      "        \"use_sequential\": 1,\n",
      "        \"warmup_proportion\": 0.2843\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-05-05-08-49-07-014\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sagemaker_smp_pretrain\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.8xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.8xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sagemaker_smp_pretrain.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sagemaker_smp_pretrain.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose --mca orte_base_help_aggregate 0 \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sagemaker_smp_pretrain\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose --mca orte_base_help_aggregate 0 \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2023-05-05-08-49-07-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_smp_pretrain\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_smp_pretrain.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--allreduce_post_accumulation\",\"1\",\"--allreduce_post_accumulation_fp16\",\"1\",\"--apply_optimizer\",\"1\",\"--bert_model\",\"bert-large-uncased\",\"--config_file\",\"bert_config.json\",\"--do_train\",\"1\",\"--input_dir\",\"/opt/ml/input/data/train\",\"--learning_rate\",\"0.006\",\"--max_predictions_per_seq\",\"20\",\"--max_seq_length\",\"128\",\"--max_steps\",\"7038\",\"--mp_parameters\",\"ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster\",\"--num_steps_per_checkpoint\",\"200\",\"--output_dir\",\"./checkpoints\",\"--seed\",\"12439\",\"--skip_checkpoint\",\"1\",\"--smp\",\"1\",\"--steps_this_run\",\"500\",\"--train_batch_size\",\"48\",\"--use_sequential\",\"1\",\"--warmup_proportion\",\"0.2843\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_ALLREDUCE_POST_ACCUMULATION=1\u001b[0m\n",
      "\u001b[34mSM_HP_ALLREDUCE_POST_ACCUMULATION_FP16=1\u001b[0m\n",
      "\u001b[34mSM_HP_APPLY_OPTIMIZER=1\u001b[0m\n",
      "\u001b[34mSM_HP_BERT_MODEL=bert-large-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG_FILE=bert_config.json\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=1\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_DIR=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.006\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_PREDICTIONS_PER_SEQ=20\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=7038\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_STEPS_PER_CHECKPOINT=200\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=./checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=12439\u001b[0m\n",
      "\u001b[34mSM_HP_SKIP_CHECKPOINT=1\u001b[0m\n",
      "\u001b[34mSM_HP_SMP=1\u001b[0m\n",
      "\u001b[34mSM_HP_STEPS_THIS_RUN=500\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=48\u001b[0m\n",
      "\u001b[34mSM_HP_USE_SEQUENTIAL=1\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_PROPORTION=0.2843\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose --mca orte_base_help_aggregate 0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAIN -x SM_HP_ALLREDUCE_POST_ACCUMULATION -x SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16 -x SM_HP_APPLY_OPTIMIZER -x SM_HP_BERT_MODEL -x SM_HP_CONFIG_FILE -x SM_HP_DO_TRAIN -x SM_HP_INPUT_DIR -x SM_HP_LEARNING_RATE -x SM_HP_MAX_PREDICTIONS_PER_SEQ -x SM_HP_MAX_SEQ_LENGTH -x SM_HP_MAX_STEPS -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_STEPS_PER_CHECKPOINT -x SM_HP_OUTPUT_DIR -x SM_HP_SEED -x SM_HP_SKIP_CHECKPOINT -x SM_HP_SMP -x SM_HP_STEPS_THIS_RUN -x SM_HP_TRAIN_BATCH_SIZE -x SM_HP_USE_SEQUENTIAL -x SM_HP_WARMUP_PROPORTION -x PYTHONPATH /opt/conda/bin/python3.6 -m mpi4py sagemaker_smp_pretrain.py --allreduce_post_accumulation 1 --allreduce_post_accumulation_fp16 1 --apply_optimizer 1 --bert_model bert-large-uncased --config_file bert_config.json --do_train 1 --input_dir /opt/ml/input/data/train --learning_rate 0.006 --max_predictions_per_seq 20 --max_seq_length 128 --max_steps 7038 --mp_parameters ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster --num_steps_per_checkpoint 200 --output_dir ./checkpoints --seed 12439 --skip_checkpoint 1 --smp 1 --steps_this_run 500 --train_batch_size 48 --use_sequential 1 --warmup_proportion 0.2843\n",
      " Data for JOB [41167,1] offset 0 Total slots allocated 8\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 7 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:{'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-5e3c62bc095f77609317142ee3696e0d3eebe9bcb073bca17a438a61a9fb9a15-customer',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'AWS_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'CMAKE_PREFIX_PATH': '$(dirname '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                      '$(which '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                      'conda))/../',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'CUDA_VERSION': '11.0.3',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'CUDNN_VERSION': '8.0.4.30',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'DGLBACKEND': 'pytorch',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'DMLC_INTERFACE': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'HFI_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'HOME': '/root',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'HOROVOD_VERSION': '0.20.3',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'HOSTNAME': 'ip-10-0-167-116.us-east-2.compute.internal',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'IPATH_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'LANG': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'LC_ALL': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'LD_LIBRARY_PATH': '/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib:/home/.openmpi/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'LD_PRELOAD': '/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'MANUAL_BUILD': '0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'MASTER_ADDR': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'MASTER_PORT': '7777',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'NCCL_DEBUG': 'INFO',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'NCCL_IB_DISABLE': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'NCCL_MIN_NRINGS': '4',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'NCCL_SOCKET_IFNAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'NCCL_VERSION': '2.7.8',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'NVIDIA_REQUIRE_CUDA': 'cuda>=11.0 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                        'brand=tesla,driver>=418,driver<419 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                        'brand=tesla,driver>=440,driver<441 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                        'brand=tesla,driver>=450,driver<451',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'NVIDIA_VISIBLE_DEVICES': 'all',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_APP_CTX_NUM_PROCS': '8',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_ARGV': '-m '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              'mpi4py '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              'sagemaker_smp_pretrain.py '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--allreduce_post_accumulation '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              [1,6]<stdout>:'--allreduce_post_accumulation_fp16 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--apply_optimizer '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              [1,6]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--bert_model '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              'bert-large-uncased '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--config_file '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              'bert_config.json '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--do_train '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              [1,6]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--input_dir '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '/opt/ml/input/data/train '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--learning_rate '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '0.006 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--max_predictions_per_seq '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '20 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              [1,6]<stdout>:'--max_seq_length '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '128 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--max_steps '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              [1,6]<stdout>:'7038 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--mp_parameters '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              [1,6]<stdout>:'ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--num_steps_per_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '200 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--output_dir '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              './checkpoints '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--seed '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '12439 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              [1,6]<stdout>:'--skip_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              [1,6]<stdout>:'--smp '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--steps_this_run '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '500 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--train_batch_size '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '48 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--use_sequential '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              [1,6]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              '--warmup_proportion '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:              [1,6]<stdout>:'0.2843',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_COMMAND': 'python3.6',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_COMM_WORLD_LOCAL_RANK': '6',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_COMM_WORLD_LOCAL_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_COMM_WORLD_NODE_RANK': '6',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_COMM_WORLD_RANK': '6',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_COMM_WORLD_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_FILE_LOCATION': '/tmp/ompi.algo-1.0/pid.41/0/0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_FIRST_RANKS': '0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_btl': '^openib',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_btl_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_ess': '^singleton',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_ess_base_jobid': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_ess_base_vpid': '6',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_hwloc_base_binding_policy': 'none',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_initial_wdir': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_mpi_yield_when_idle': '0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_oob_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_orte_abort_on_non_zero_status': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_orte_app_num': '0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_orte_base_help_aggregate': '0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_orte_ess_node_rank': '6',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_orte_ess_num_procs': '8',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_orte_hnp_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_orte_launch': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_orte_local_daemon_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_orte_num_nodes': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_orte_precondition_transports': '99cb0754aa46131b-b765a7ffda563ac1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_orte_tag_output': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_orte_tmpdir_base': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.algo-1.0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_plm_rsh_no_tree_spawn': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_pmix': '^s1,s2,cray,isolated',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_pml': 'ob1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_rmaps_base_display_map': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_MCA_rmaps_base_mapping_policy': 'slot',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'OMPI_NUM_APP_CTX': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'OMPI_UNIVERSE_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'PATH': '/home/.openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds21_41',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds12_41',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PMIX_GDS_MODULE': 'ds21,ds12,hash',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'PMIX_ID': '2697920513.6',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PMIX_MCA_mca_base_component_show_load_errors': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'PMIX_NAMESPACE': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PMIX_PTL_MODULE': 'tcp,usock',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'PMIX_RANK': '6',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PMIX_SECURITY_MODE': 'native',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'PMIX_SERVER_TMPDIR': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PMIX_SERVER_URI2': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'PMIX_SERVER_URI21': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PMIX_SERVER_URI3': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'PMIX_SYSTEM_TMPDIR': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PWD': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PYTHONDONTWRITEBYTECODE': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'PYTHONIOENCODING': 'UTF-8',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'PYTHONUNBUFFERED': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SAGEMAKER_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SAGEMAKER_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SHLVL': '2',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_CHANNELS': '[\"train\"]',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_FRAMEWORK_PARAMS': '{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                        '--mca '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                        'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                        [1,6]<stdout>:'0 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                        '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_HOSTS': '[\"algo-1\"]',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HPS': '{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843}',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_APPLY_OPTIMIZER': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_HP_BERT_MODEL': 'bert-large-uncased',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_CONFIG_FILE': 'bert_config.json',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_HP_DO_TRAIN': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_INPUT_DIR': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_LEARNING_RATE': '0.006',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_HP_MAX_PREDICTIONS_PER_SEQ': '20',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_MAX_SEQ_LENGTH': '128',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_HP_MAX_STEPS': '7038',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_MP_PARAMETERS': '{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_HP_NUM_STEPS_PER_CHECKPOINT': '200',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_OUTPUT_DIR': './checkpoints',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_SEED': '12439',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_HP_SKIP_CHECKPOINT': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_SMP': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_STEPS_THIS_RUN': '500',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_TRAIN_BATCH_SIZE': '48',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_HP_USE_SEQUENTIAL': '1',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_HP_WARMUP_PROPORTION': '0.2843',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_INPUT_DATA_CONFIG': '{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_INPUT_DIR': '/opt/ml/input',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_LOG_LEVEL': '20',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_MODEL_DIR': '/opt/ml/model',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_MODULE_DIR': 's3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_MODULE_NAME': 'sagemaker_smp_pretrain',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_NETWORK_INTERFACE_NAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_NUM_CPUS': '32',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_NUM_GPUS': '4',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_OUTPUT_DIR': '/opt/ml/output',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                    '--mca '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                    'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                    '0 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                    [1,6]<stdout>:'\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2023-05-05-08-49-07-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_smp_pretrain\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_smp_pretrain.py\"}',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'SM_USER_ARGS': '[\"--allreduce_post_accumulation\",\"1\",\"--allreduce_post_accumulation_fp16\",\"1\",\"--apply_optimizer\",\"1\",\"--bert_model\",\"bert-large-uncased\",\"--config_file\",\"bert_config.json\",\"--do_train\",\"1\",\"--input_dir\",\"/opt/ml/input/data/train\",\"--learning_rate\",\"0.006\",\"--max_predictions_per_seq\",\"20\",\"--max_seq_length\",\"128\",\"--max_steps\",\"7038\",\"--mp_parameters\",\"ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster\",\"--num_steps_per_checkpoint\",\"200\",\"--output_dir\",\"./checkpoints\",\"--seed\",\"12439\",\"--skip_checkpoint\",\"1\",\"--smp\",\"1\",\"--steps_this_run\",\"500\",\"--train_batch_size\",\"48\",\"--use_sequential\",\"1\",\"--warmup_proportion\",\"0.2843\"]',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'SM_USER_ENTRY_POINT': 'sagemaker_smp_pretrain.py',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'TORCH_CUDA_ARCH_LIST': '3.5 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                         '3.7 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                         '5.2 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                         '6.0 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                         '6.1 '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                         [1,6]<stdout>:'7.0+PTX '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                         '8.0',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'TORCH_NVCC_FLAGS': '-Xfatbin '\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:                     '-compress-all',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:601636808299:training-job/pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: [1,6]<stdout>:'TRAINING_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: '_': '/home/.openmpi/bin/mpirun.real'}\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:{'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-5e3c62bc095f77609317142ee3696e0d3eebe9bcb073bca17a438a61a9fb9a15-customer',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'AWS_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'CMAKE_PREFIX_PATH': '$(dirname '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                      '$(which '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                      'conda))/../',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'CUDA_VERSION': '11.0.3',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'CUDNN_VERSION': '8.0.4.30',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'DGLBACKEND': 'pytorch',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'DMLC_INTERFACE': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'HFI_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'HOME': '/root',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'HOROVOD_VERSION': '0.20.3',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'HOSTNAME': 'ip-10-0-167-116.us-east-2.compute.internal',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'IPATH_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'LANG': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'LC_ALL': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'LD_LIBRARY_PATH': '/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib:/home/.openmpi/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'LD_PRELOAD': '/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'MANUAL_BUILD': '0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'MASTER_ADDR': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'MASTER_PORT': '7777',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'NCCL_DEBUG': 'INFO',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'NCCL_IB_DISABLE': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'NCCL_MIN_NRINGS': '4',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'NCCL_SOCKET_IFNAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'NCCL_VERSION': '2.7.8',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'NVIDIA_REQUIRE_CUDA': 'cuda>=11.0 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                        'brand=tesla,driver>=418,driver<419 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                        'brand=tesla,driver>=440,driver<441 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                        'brand=tesla,driver>=450,driver<451',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'NVIDIA_VISIBLE_DEVICES': 'all',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_APP_CTX_NUM_PROCS': '8',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_ARGV': '-m '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              'mpi4py '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              'sagemaker_smp_pretrain.py '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--allreduce_post_accumulation '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--allreduce_post_accumulation_fp16 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--apply_optimizer '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              [1,4]<stdout>:'--bert_model '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              'bert-large-uncased '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              [1,4]<stdout>:'--config_file '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              'bert_config.json '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--do_train '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--input_dir '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              [1,4]<stdout>:'/opt/ml/input/data/train '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--learning_rate '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '0.006 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--max_predictions_per_seq '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '20 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--max_seq_length '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '128 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--max_steps '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '7038 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              [1,4]<stdout>:'--mp_parameters '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              'ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--num_steps_per_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '200 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--output_dir '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              './checkpoints '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              [1,4]<stdout>:'--seed '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '12439 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--skip_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              [1,4]<stdout>:'--smp '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--steps_this_run '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '500 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--train_batch_size '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '48 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              [1,4]<stdout>:'--use_sequential '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '--warmup_proportion '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:              '0.2843',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_COMMAND': 'python3.6',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_COMM_WORLD_LOCAL_RANK': '4',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_COMM_WORLD_LOCAL_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_COMM_WORLD_NODE_RANK': '4',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_COMM_WORLD_RANK': '4',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_COMM_WORLD_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_FILE_LOCATION': '/tmp/ompi.algo-1.0/pid.41/0/0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_FIRST_RANKS': '0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_btl': '^openib',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_btl_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_ess': '^singleton',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_ess_base_jobid': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_ess_base_vpid': '4',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_hwloc_base_binding_policy': 'none',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_initial_wdir': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_mpi_yield_when_idle': '0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_oob_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_orte_abort_on_non_zero_status': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_orte_app_num': '0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_orte_base_help_aggregate': '0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_orte_ess_node_rank': '4',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_orte_ess_num_procs': '8',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_orte_hnp_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_orte_launch': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_orte_local_daemon_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_orte_num_nodes': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_orte_precondition_transports': '99cb0754aa46131b-b765a7ffda563ac1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_orte_tag_output': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_orte_tmpdir_base': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.algo-1.0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_plm_rsh_no_tree_spawn': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_pmix': '^s1,s2,cray,isolated',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_pml': 'ob1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_rmaps_base_display_map': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_MCA_rmaps_base_mapping_policy': 'slot',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'OMPI_NUM_APP_CTX': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'OMPI_UNIVERSE_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PATH': '/home/.openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds21_41',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds12_41',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'PMIX_GDS_MODULE': 'ds21,ds12,hash',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'PMIX_ID': '2697920513.4',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PMIX_MCA_mca_base_component_show_load_errors': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PMIX_NAMESPACE': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'PMIX_PTL_MODULE': 'tcp,usock',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PMIX_RANK': '4',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'PMIX_SECURITY_MODE': 'native',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'PMIX_SERVER_TMPDIR': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'PMIX_SERVER_URI2': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PMIX_SERVER_URI21': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PMIX_SERVER_URI3': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'PMIX_SYSTEM_TMPDIR': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PWD': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'PYTHONDONTWRITEBYTECODE': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PYTHONIOENCODING': 'UTF-8',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'PYTHONUNBUFFERED': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SAGEMAKER_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SAGEMAKER_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SHLVL': '2',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_CHANNELS': '[\"train\"]',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_FRAMEWORK_PARAMS': '{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                        '--mca '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                        'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                        '0 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                        '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HOSTS': '[\"algo-1\"]',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HPS': '{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843}',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_ALLREDUCE_POST_ACCUMULATION': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_APPLY_OPTIMIZER': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_HP_BERT_MODEL': 'bert-large-uncased',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_CONFIG_FILE': 'bert_config.json',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_HP_DO_TRAIN': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_HP_INPUT_DIR': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_LEARNING_RATE': '0.006',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_HP_MAX_PREDICTIONS_PER_SEQ': '20',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_HP_MAX_SEQ_LENGTH': '128',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_MAX_STEPS': '7038',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_HP_MP_PARAMETERS': '{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_NUM_STEPS_PER_CHECKPOINT': '200',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_OUTPUT_DIR': './checkpoints',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_SEED': '12439',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_SKIP_CHECKPOINT': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_SMP': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_HP_STEPS_THIS_RUN': '500',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_HP_TRAIN_BATCH_SIZE': '48',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_HP_USE_SEQUENTIAL': '1',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_HP_WARMUP_PROPORTION': '0.2843',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_INPUT_DATA_CONFIG': '{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_INPUT_DIR': '/opt/ml/input',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_LOG_LEVEL': '20',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_MODEL_DIR': '/opt/ml/model',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_MODULE_DIR': 's3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_MODULE_NAME': 'sagemaker_smp_pretrain',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_NETWORK_INTERFACE_NAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_NUM_CPUS': '32',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_NUM_GPUS': '4',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_OUTPUT_DIR': '/opt/ml/output',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                    '--mca '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                    'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                    '0 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                    '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2023-05-05-08-49-07-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_smp_pretrain\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_smp_pretrain.py\"}',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'SM_USER_ARGS': '[\"--allreduce_post_accumulation\",\"1\",\"--allreduce_post_accumulation_fp16\",\"1\",\"--apply_optimizer\",\"1\",\"--bert_model\",\"bert-large-uncased\",\"--config_file\",\"bert_config.json\",\"--do_train\",\"1\",\"--input_dir\",\"/opt/ml/input/data/train\",\"--learning_rate\",\"0.006\",\"--max_predictions_per_seq\",\"20\",\"--max_seq_length\",\"128\",\"--max_steps\",\"7038\",\"--mp_parameters\",\"ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster\",\"--num_steps_per_checkpoint\",\"200\",\"--output_dir\",\"./checkpoints\",\"--seed\",\"12439\",\"--skip_checkpoint\",\"1\",\"--smp\",\"1\",\"--steps_this_run\",\"500\",\"--train_batch_size\",\"48\",\"--use_sequential\",\"1\",\"--warmup_proportion\",\"0.2843\"]',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'SM_USER_ENTRY_POINT': 'sagemaker_smp_pretrain.py',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'TORCH_CUDA_ARCH_LIST': '3.5 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                         '3.7 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                         '5.2 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                         '6.0 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                         '6.1 '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                         '7.0+PTX '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                         '8.0',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'TORCH_NVCC_FLAGS': '-Xfatbin '\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:                     '-compress-all',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:601636808299:training-job/pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'TRAINING_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:'_': '/home/.openmpi/bin/mpirun.real'}\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:{'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-5e3c62bc095f77609317142ee3696e0d3eebe9bcb073bca17a438a61a9fb9a15-customer',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'AWS_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'CMAKE_PREFIX_PATH': '$(dirname '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                      '$(which '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                      'conda))/../',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'CUDA_VERSION': '11.0.3',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'CUDNN_VERSION': '8.0.4.30',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'DGLBACKEND': 'pytorch',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'DMLC_INTERFACE': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'HFI_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'HOME': '/root',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'HOROVOD_VERSION': '0.20.3',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'HOSTNAME': 'ip-10-0-167-116.us-east-2.compute.internal',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'IPATH_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'LANG': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'LC_ALL': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib:/home/.openmpi/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'LD_PRELOAD': '/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'MANUAL_BUILD': '0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'MASTER_ADDR': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'MASTER_PORT': '7777',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'NCCL_DEBUG': 'INFO',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'NCCL_IB_DISABLE': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'NCCL_MIN_NRINGS': '4',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'NCCL_SOCKET_IFNAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'NCCL_VERSION': '2.7.8',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'NVIDIA_REQUIRE_CUDA': 'cuda>=11.0 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                        'brand=tesla,driver>=418,driver<419 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                        'brand=tesla,driver>=440,driver<441 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                        'brand=tesla,driver>=450,driver<451',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'NVIDIA_VISIBLE_DEVICES': 'all',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_APP_CTX_NUM_PROCS': '8',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_ARGV': '-m '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              'mpi4py '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              'sagemaker_smp_pretrain.py '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--allreduce_post_accumulation '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--allreduce_post_accumulation_fp16 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--apply_optimizer '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              [1,2]<stdout>:'--bert_model '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              'bert-large-uncased '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--config_file '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              'bert_config.json '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--do_train '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--input_dir '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '/opt/ml/input/data/train '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--learning_rate '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '0.006 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              [1,2]<stdout>:'--max_predictions_per_seq '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '20 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--max_seq_length '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '128 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--max_steps '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '7038 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              [1,2]<stdout>:'--mp_parameters '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              [1,2]<stdout>:'ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--num_steps_per_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '200 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--output_dir '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              './checkpoints '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--seed '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              [1,2]<stdout>:'12439 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              [1,2]<stdout>:'--skip_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--smp '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--steps_this_run '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '500 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--train_batch_size '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '48 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--use_sequential '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              [1,2]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '--warmup_proportion '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:              '0.2843',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_COMMAND': 'python3.6',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_COMM_WORLD_LOCAL_RANK': '2',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_COMM_WORLD_LOCAL_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_COMM_WORLD_NODE_RANK': '2',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_COMM_WORLD_RANK': '2',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_COMM_WORLD_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_FILE_LOCATION': '/tmp/ompi.algo-1.0/pid.41/0/0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_FIRST_RANKS': '0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_btl': '^openib',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_btl_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_btl_vader_single_copy_mechanism': 'none',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_ess': '^singleton',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_ess_base_jobid': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_ess_base_vpid': '2',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_hwloc_base_binding_policy': 'none',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_initial_wdir': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_mpi_yield_when_idle': '0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_oob_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_orte_abort_on_non_zero_status': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_orte_app_num': '0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_orte_base_help_aggregate': '0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_orte_ess_node_rank': '2',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_orte_ess_num_procs': '8',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_orte_hnp_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_orte_launch': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_orte_local_daemon_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_orte_num_nodes': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_orte_precondition_transports': '99cb0754aa46131b-b765a7ffda563ac1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_orte_tag_output': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_orte_tmpdir_base': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.algo-1.0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_plm_rsh_no_tree_spawn': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_pmix': '^s1,s2,cray,isolated',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_pml': 'ob1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_rmaps_base_display_map': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_MCA_rmaps_base_mapping_policy': 'slot',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'OMPI_NUM_APP_CTX': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'OMPI_UNIVERSE_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PATH': '/home/.openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds21_41',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds12_41',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'PMIX_GDS_MODULE': 'ds21,ds12,hash',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PMIX_ID': '2697920513.2',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PMIX_MCA_mca_base_component_show_load_errors': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'PMIX_NAMESPACE': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PMIX_PTL_MODULE': 'tcp,usock',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'PMIX_RANK': '2',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'PMIX_SECURITY_MODE': 'native',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PMIX_SERVER_TMPDIR': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'PMIX_SERVER_URI2': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PMIX_SERVER_URI21': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PMIX_SERVER_URI3': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'PMIX_SYSTEM_TMPDIR': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PWD': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'PYTHONDONTWRITEBYTECODE': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'PYTHONIOENCODING': 'UTF-8',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'PYTHONUNBUFFERED': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SAGEMAKER_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SAGEMAKER_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SHLVL': '2',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_CHANNELS': '[\"train\"]',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_FRAMEWORK_PARAMS': '{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                        '--mca '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                        'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                        '0 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                        '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HOSTS': '[\"algo-1\"]',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HPS': '{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843}',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_APPLY_OPTIMIZER': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_BERT_MODEL': 'bert-large-uncased',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_HP_CONFIG_FILE': 'bert_config.json',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_DO_TRAIN': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_INPUT_DIR': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_HP_LEARNING_RATE': '0.006',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_MAX_PREDICTIONS_PER_SEQ': '20',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_MAX_SEQ_LENGTH': '128',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_MAX_STEPS': '7038',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_HP_MP_PARAMETERS': '{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_NUM_STEPS_PER_CHECKPOINT': '200',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_HP_OUTPUT_DIR': './checkpoints',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_SEED': '12439',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_SKIP_CHECKPOINT': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_SMP': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_HP_STEPS_THIS_RUN': '500',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_TRAIN_BATCH_SIZE': '48',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_HP_USE_SEQUENTIAL': '1',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_HP_WARMUP_PROPORTION': '0.2843',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_INPUT_DATA_CONFIG': '{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_INPUT_DIR': '/opt/ml/input',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_LOG_LEVEL': '20',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_MODEL_DIR': '/opt/ml/model',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_MODULE_DIR': 's3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_MODULE_NAME': 'sagemaker_smp_pretrain',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_NETWORK_INTERFACE_NAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_NUM_CPUS': '32',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_NUM_GPUS': '4',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_OUTPUT_DIR': '/opt/ml/output',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                    '--mca '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                    'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                    '0 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                    [1,2]<stdout>:'\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2023-05-05-08-49-07-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_smp_pretrain\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_smp_pretrain.py\"}',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'SM_USER_ARGS': '[\"--allreduce_post_accumulation\",\"1\",\"--allreduce_post_accumulation_fp16\",\"1\",\"--apply_optimizer\",\"1\",\"--bert_model\",\"bert-large-uncased\",\"--config_file\",\"bert_config.json\",\"--do_train\",\"1\",\"--input_dir\",\"/opt/ml/input/data/train\",\"--learning_rate\",\"0.006\",\"--max_predictions_per_seq\",\"20\",\"--max_seq_length\",\"128\",\"--max_steps\",\"7038\",\"--mp_parameters\",\"ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster\",\"--num_steps_per_checkpoint\",\"200\",\"--output_dir\",\"./checkpoints\",\"--seed\",\"12439\",\"--skip_checkpoint\",\"1\",\"--smp\",\"1\",\"--steps_this_run\",\"500\",\"--train_batch_size\",\"48\",\"--use_sequential\",\"1\",\"--warmup_proportion\",\"0.2843\"]',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'SM_USER_ENTRY_POINT': 'sagemaker_smp_pretrain.py',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'TORCH_CUDA_ARCH_LIST': '3.5 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                         '3.7 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                         '5.2 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                         [1,2]<stdout>:'6.0 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                         '6.1 '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                         '7.0+PTX '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                         '8.0',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'TORCH_NVCC_FLAGS': '-Xfatbin '\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:                     '-compress-all',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:601636808299:training-job/pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: [1,2]<stdout>:'TRAINING_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: '_': '/home/.openmpi/bin/mpirun.real'}\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:{'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-5e3c62bc095f77609317142ee3696e0d3eebe9bcb073bca17a438a61a9fb9a15-customer',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'AWS_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'CMAKE_PREFIX_PATH': '$(dirname '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                      '$(which '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                      [1,7]<stdout>:'conda))/../',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'CUDA_VERSION': '11.0.3',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'CUDNN_VERSION': '8.0.4.30',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'DGLBACKEND': 'pytorch',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'DMLC_INTERFACE': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'HFI_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'HOME': '/root',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'HOROVOD_VERSION': '0.20.3',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'HOSTNAME': 'ip-10-0-167-116.us-east-2.compute.internal',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'IPATH_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'LANG': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'LC_ALL': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'LD_LIBRARY_PATH': '/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib:/home/.openmpi/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'LD_PRELOAD': '/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'MANUAL_BUILD': '0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'MASTER_ADDR': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'MASTER_PORT': '7777',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'NCCL_DEBUG': 'INFO',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'NCCL_IB_DISABLE': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'NCCL_MIN_NRINGS': '4',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'NCCL_SOCKET_IFNAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'NCCL_VERSION': '2.7.8',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.0 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                        'brand=tesla,driver>=418,driver<419 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                        'brand=tesla,driver>=440,driver<441 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                        [1,7]<stdout>:'brand=tesla,driver>=450,driver<451',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'NVIDIA_VISIBLE_DEVICES': 'all',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_APP_CTX_NUM_PROCS': '8',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_ARGV': '-m '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              'mpi4py '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              'sagemaker_smp_pretrain.py '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--allreduce_post_accumulation '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--allreduce_post_accumulation_fp16 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--apply_optimizer '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--bert_model '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              'bert-large-uncased '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--config_file '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'bert_config.json '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--do_train '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'--input_dir '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '/opt/ml/input/data/train '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--learning_rate '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '0.006 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--max_predictions_per_seq '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '20 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'--max_seq_length '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '128 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'--max_steps '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '7038 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--mp_parameters '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              'ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'--num_steps_per_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '200 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'--output_dir '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              './checkpoints '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'--seed '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '12439 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--skip_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--smp '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--steps_this_run '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'500 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--train_batch_size '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '48 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--use_sequential '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              '--warmup_proportion '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:              [1,7]<stdout>:'0.2843',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_COMMAND': 'python3.6',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_COMM_WORLD_LOCAL_RANK': '7',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_COMM_WORLD_LOCAL_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_COMM_WORLD_NODE_RANK': '7',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_COMM_WORLD_RANK': '7',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_COMM_WORLD_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_FILE_LOCATION': '/tmp/ompi.algo-1.0/pid.41/0/0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_FIRST_RANKS': '0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_btl': '^openib',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_btl_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_btl_vader_single_copy_mechanism': 'none',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_ess': '^singleton',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_ess_base_jobid': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_ess_base_vpid': '7',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_hwloc_base_binding_policy': 'none',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_initial_wdir': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_mpi_yield_when_idle': '0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_oob_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_orte_abort_on_non_zero_status': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_orte_app_num': '0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_orte_base_help_aggregate': '0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_orte_ess_node_rank': '7',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_orte_ess_num_procs': '8',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_orte_hnp_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_orte_launch': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_orte_local_daemon_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_orte_num_nodes': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_orte_precondition_transports': '99cb0754aa46131b-b765a7ffda563ac1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_orte_tag_output': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_orte_tmpdir_base': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.algo-1.0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_plm_rsh_no_tree_spawn': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_pmix': '^s1,s2,cray,isolated',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_pml': 'ob1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_rmaps_base_display_map': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_MCA_rmaps_base_mapping_policy': 'slot',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'OMPI_NUM_APP_CTX': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'OMPI_UNIVERSE_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PATH': '/home/.openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds21_41',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds12_41',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PMIX_GDS_MODULE': 'ds21,ds12,hash',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'PMIX_ID': '2697920513.7',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PMIX_MCA_mca_base_component_show_load_errors': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'PMIX_NAMESPACE': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PMIX_PTL_MODULE': 'tcp,usock',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'PMIX_RANK': '7',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PMIX_SECURITY_MODE': 'native',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'PMIX_SERVER_TMPDIR': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PMIX_SERVER_URI2': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'PMIX_SERVER_URI21': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PMIX_SERVER_URI3': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'PMIX_SYSTEM_TMPDIR': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PWD': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'PYTHONDONTWRITEBYTECODE': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PYTHONIOENCODING': 'UTF-8',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'PYTHONUNBUFFERED': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SAGEMAKER_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SAGEMAKER_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SHLVL': '2',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_CHANNELS': '[\"train\"]',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_FRAMEWORK_PARAMS': '{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                        '--mca '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                        'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                        '0 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                        '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HOSTS': '[\"algo-1\"]',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HPS': '{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843}',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_ALLREDUCE_POST_ACCUMULATION': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_APPLY_OPTIMIZER': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_HP_BERT_MODEL': 'bert-large-uncased',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_CONFIG_FILE': 'bert_config.json',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_HP_DO_TRAIN': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_INPUT_DIR': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_LEARNING_RATE': '0.006',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_HP_MAX_PREDICTIONS_PER_SEQ': '20',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_MAX_SEQ_LENGTH': '128',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_HP_MAX_STEPS': '7038',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_MP_PARAMETERS': '{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_HP_NUM_STEPS_PER_CHECKPOINT': '200',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_OUTPUT_DIR': './checkpoints',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_HP_SEED': '12439',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_SKIP_CHECKPOINT': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_HP_SMP': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_STEPS_THIS_RUN': '500',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_HP_TRAIN_BATCH_SIZE': '48',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_HP_USE_SEQUENTIAL': '1',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_HP_WARMUP_PROPORTION': '0.2843',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_INPUT_DATA_CONFIG': '{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_INPUT_DIR': '/opt/ml/input',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_LOG_LEVEL': '20',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_MODEL_DIR': '/opt/ml/model',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_MODULE_DIR': 's3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_MODULE_NAME': 'sagemaker_smp_pretrain',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_NETWORK_INTERFACE_NAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_NUM_CPUS': '32',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_NUM_GPUS': '4',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_OUTPUT_DIR': '/opt/ml/output',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                    '--mca '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                    'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                    '0 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                    '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2023-05-05-08-49-07-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_smp_pretrain\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_smp_pretrain.py\"}',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'SM_USER_ARGS': '[\"--allreduce_post_accumulation\",\"1\",\"--allreduce_post_accumulation_fp16\",\"1\",\"--apply_optimizer\",\"1\",\"--bert_model\",\"bert-large-uncased\",\"--config_file\",\"bert_config.json\",\"--do_train\",\"1\",\"--input_dir\",\"/opt/ml/input/data/train\",\"--learning_rate\",\"0.006\",\"--max_predictions_per_seq\",\"20\",\"--max_seq_length\",\"128\",\"--max_steps\",\"7038\",\"--mp_parameters\",\"ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster\",\"--num_steps_per_checkpoint\",\"200\",\"--output_dir\",\"./checkpoints\",\"--seed\",\"12439\",\"--skip_checkpoint\",\"1\",\"--smp\",\"1\",\"--steps_this_run\",\"500\",\"--train_batch_size\",\"48\",\"--use_sequential\",\"1\",\"--warmup_proportion\",\"0.2843\"]',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'SM_USER_ENTRY_POINT': 'sagemaker_smp_pretrain.py',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'TORCH_CUDA_ARCH_LIST': '3.5 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                         '3.7 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                         '5.2 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                         [1,7]<stdout>:'6.0 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                         '6.1 '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                         '7.0+PTX '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                         '8.0',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'TORCH_NVCC_FLAGS': '-Xfatbin '\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:                     '-compress-all',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:601636808299:training-job/pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: 'TRAINING_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: [1,7]<stdout>:'_': '/home/.openmpi/bin/mpirun.real'}\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:{'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-5e3c62bc095f77609317142ee3696e0d3eebe9bcb073bca17a438a61a9fb9a15-customer',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'AWS_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'CMAKE_PREFIX_PATH': '$(dirname '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                      '$(which '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                      'conda))/../',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'CUDA_VERSION': '11.0.3',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'CUDNN_VERSION': '8.0.4.30',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'DGLBACKEND': 'pytorch',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'DMLC_INTERFACE': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'HFI_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'HOME': '/root',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'HOROVOD_VERSION': '0.20.3',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'HOSTNAME': 'ip-10-0-167-116.us-east-2.compute.internal',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'IPATH_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'LANG': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'LC_ALL': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'LD_LIBRARY_PATH': '/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib:/home/.openmpi/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'LD_PRELOAD': '/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'MANUAL_BUILD': '0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'MASTER_ADDR': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'MASTER_PORT': '7777',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'NCCL_DEBUG': 'INFO',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'NCCL_IB_DISABLE': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'NCCL_MIN_NRINGS': '4',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'NCCL_SOCKET_IFNAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'NCCL_VERSION': '2.7.8',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'NVIDIA_REQUIRE_CUDA': 'cuda>=11.0 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                        'brand=tesla,driver>=418,driver<419 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                        'brand=tesla,driver>=440,driver<441 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                        'brand=tesla,driver>=450,driver<451',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'NVIDIA_VISIBLE_DEVICES': 'all',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_APP_CTX_NUM_PROCS': '8',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_ARGV': '-m '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              'mpi4py '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              'sagemaker_smp_pretrain.py '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--allreduce_post_accumulation '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--allreduce_post_accumulation_fp16 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              [1,1]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--apply_optimizer '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              [1,1]<stdout>:'--bert_model '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              'bert-large-uncased '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              [1,1]<stdout>:'--config_file '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              'bert_config.json '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--do_train '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--input_dir '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              [1,1]<stdout>:'/opt/ml/input/data/train '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--learning_rate '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '0.006 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--max_predictions_per_seq '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '20 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--max_seq_length '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '128 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--max_steps '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '7038 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--mp_parameters '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              [1,1]<stdout>:'ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--num_steps_per_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '200 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--output_dir '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              './checkpoints '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--seed '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '12439 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--skip_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--smp '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              [1,1]<stdout>:'--steps_this_run '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '500 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--train_batch_size '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              [1,1]<stdout>:'48 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--use_sequential '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '--warmup_proportion '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:              '0.2843',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_COMMAND': 'python3.6',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_COMM_WORLD_LOCAL_RANK': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_COMM_WORLD_LOCAL_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_COMM_WORLD_NODE_RANK': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_COMM_WORLD_RANK': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_COMM_WORLD_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_FILE_LOCATION': '/tmp/ompi.algo-1.0/pid.41/0/0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_FIRST_RANKS': '0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_btl': '^openib',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_btl_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_btl_vader_single_copy_mechanism': 'none',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_ess': '^singleton',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_ess_base_jobid': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_ess_base_vpid': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_hwloc_base_binding_policy': 'none',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_initial_wdir': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_mpi_yield_when_idle': '0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_oob_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_orte_abort_on_non_zero_status': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_orte_app_num': '0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_orte_base_help_aggregate': '0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_orte_ess_node_rank': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_orte_ess_num_procs': '8',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_orte_hnp_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_orte_launch': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_orte_local_daemon_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_orte_num_nodes': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_orte_precondition_transports': '99cb0754aa46131b-b765a7ffda563ac1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_orte_tag_output': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_orte_tmpdir_base': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.algo-1.0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_plm_rsh_no_tree_spawn': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_pmix': '^s1,s2,cray,isolated',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_pml': 'ob1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_rmaps_base_display_map': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_MCA_rmaps_base_mapping_policy': 'slot',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'OMPI_NUM_APP_CTX': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'OMPI_UNIVERSE_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PATH': '/home/.openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds21_41',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds12_41',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PMIX_GDS_MODULE': 'ds21,ds12,hash',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'PMIX_ID': '2697920513.1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PMIX_MCA_mca_base_component_show_load_errors': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'PMIX_NAMESPACE': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'PMIX_PTL_MODULE': 'tcp,usock',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PMIX_RANK': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'PMIX_SECURITY_MODE': 'native',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PMIX_SERVER_TMPDIR': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PMIX_SERVER_URI2': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PMIX_SERVER_URI21': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PMIX_SERVER_URI3': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'PMIX_SYSTEM_TMPDIR': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PWD': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PYTHONDONTWRITEBYTECODE': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'PYTHONIOENCODING': 'UTF-8',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'PYTHONUNBUFFERED': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SAGEMAKER_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SAGEMAKER_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SHLVL': '2',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_CHANNELS': '[\"train\"]',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                        '--mca '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                        'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                        [1,1]<stdout>:'0 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                        '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HOSTS': '[\"algo-1\"]',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HPS': '{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843}',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_APPLY_OPTIMIZER': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_HP_BERT_MODEL': 'bert-large-uncased',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_CONFIG_FILE': 'bert_config.json',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_HP_DO_TRAIN': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_INPUT_DIR': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_LEARNING_RATE': '0.006',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_MAX_PREDICTIONS_PER_SEQ': '20',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_HP_MAX_SEQ_LENGTH': '128',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_MAX_STEPS': '7038',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_HP_MP_PARAMETERS': '{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_NUM_STEPS_PER_CHECKPOINT': '200',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_HP_OUTPUT_DIR': './checkpoints',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_SEED': '12439',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_HP_SKIP_CHECKPOINT': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_SMP': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_HP_STEPS_THIS_RUN': '500',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_TRAIN_BATCH_SIZE': '48',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_HP_USE_SEQUENTIAL': '1',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_HP_WARMUP_PROPORTION': '0.2843',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_INPUT_DATA_CONFIG': '{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_INPUT_DIR': '/opt/ml/input',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_LOG_LEVEL': '20',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_MODEL_DIR': '/opt/ml/model',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_MODULE_DIR': 's3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_MODULE_NAME': 'sagemaker_smp_pretrain',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_NETWORK_INTERFACE_NAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_NUM_CPUS': '32',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_NUM_GPUS': '4',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_OUTPUT_DIR': '/opt/ml/output',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                    '--mca '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                    'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                    '0 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                    '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2023-05-05-08-49-07-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_smp_pretrain\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_smp_pretrain.py\"}',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_USER_ARGS': '[\"--allreduce_post_accumulation\",\"1\",\"--allreduce_post_accumulation_fp16\",\"1\",\"--apply_optimizer\",\"1\",\"--bert_model\",\"bert-large-uncased\",\"--config_file\",\"bert_config.json\",\"--do_train\",\"1\",\"--input_dir\",\"/opt/ml/input/data/train\",\"--learning_rate\",\"0.006\",\"--max_predictions_per_seq\",\"20\",\"--max_seq_length\",\"128\",\"--max_steps\",\"7038\",\"--mp_parameters\",\"ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster\",\"--num_steps_per_checkpoint\",\"200\",\"--output_dir\",\"./checkpoints\",\"--seed\",\"12439\",\"--skip_checkpoint\",\"1\",\"--smp\",\"1\",\"--steps_this_run\",\"500\",\"--train_batch_size\",\"48\",\"--use_sequential\",\"1\",\"--warmup_proportion\",\"0.2843\"]',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'SM_USER_ENTRY_POINT': 'sagemaker_smp_pretrain.py',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'TORCH_CUDA_ARCH_LIST': '3.5 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                         '3.7 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                         '5.2 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                         '6.0 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                         [1,1]<stdout>:'6.1 '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                         '7.0+PTX '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                         '8.0',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'TORCH_NVCC_FLAGS': '-Xfatbin '\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:                     '-compress-all',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:601636808299:training-job/pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: 'TRAINING_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: [1,1]<stdout>:'_': '/home/.openmpi/bin/mpirun.real'}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-5e3c62bc095f77609317142ee3696e0d3eebe9bcb073bca17a438a61a9fb9a15-customer',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'AWS_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'CMAKE_PREFIX_PATH': '$(dirname '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                      '$(which '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                      'conda))/../',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'CUDA_VERSION': '11.0.3',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'CUDNN_VERSION': '8.0.4.30',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'DGLBACKEND': 'pytorch',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'DMLC_INTERFACE': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'HFI_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'HOME': '/root',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'HOROVOD_VERSION': '0.20.3',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'HOSTNAME': 'ip-10-0-167-116.us-east-2.compute.internal',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'IPATH_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'LANG': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'LC_ALL': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'LD_LIBRARY_PATH': '/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib:/home/.openmpi/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'LD_PRELOAD': '/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'MANUAL_BUILD': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'MASTER_ADDR': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'MASTER_PORT': '7777',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'NCCL_DEBUG': 'INFO',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'NCCL_IB_DISABLE': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'NCCL_MIN_NRINGS': '4',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'NCCL_SOCKET_IFNAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'NCCL_VERSION': '2.7.8',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'NVIDIA_REQUIRE_CUDA': 'cuda>=11.0 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                        'brand=tesla,driver>=418,driver<419 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                        'brand=tesla,driver>=440,driver<441 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                        'brand=tesla,driver>=450,driver<451',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'NVIDIA_VISIBLE_DEVICES': 'all',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_APP_CTX_NUM_PROCS': '8',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_ARGV': '-m '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              'mpi4py '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              'sagemaker_smp_pretrain.py '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--allreduce_post_accumulation '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--allreduce_post_accumulation_fp16 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--apply_optimizer '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--bert_model '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              'bert-large-uncased '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              [1,0]<stdout>:'--config_file '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              'bert_config.json '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--do_train '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--input_dir '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              [1,0]<stdout>:'/opt/ml/input/data/train '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--learning_rate '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '0.006 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--max_predictions_per_seq '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              [1,0]<stdout>:'20 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--max_seq_length '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '128 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--max_steps '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              [1,0]<stdout>:'7038 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--mp_parameters '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              'ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--num_steps_per_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '200 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--output_dir '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              './checkpoints '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--seed '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '12439 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--skip_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              [1,0]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--smp '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--steps_this_run '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '500 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--train_batch_size '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '48 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--use_sequential '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '--warmup_proportion '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:              '0.2843',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_COMMAND': 'python3.6',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_COMM_WORLD_LOCAL_RANK': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_COMM_WORLD_LOCAL_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_COMM_WORLD_NODE_RANK': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_COMM_WORLD_RANK': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_COMM_WORLD_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_FILE_LOCATION': '/tmp/ompi.algo-1.0/pid.41/0/0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_FIRST_RANKS': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_btl': '^openib',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_btl_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_btl_vader_single_copy_mechanism': 'none',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_ess': '^singleton',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_ess_base_jobid': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_ess_base_vpid': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_hwloc_base_binding_policy': 'none',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_initial_wdir': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_mpi_yield_when_idle': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_oob_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_orte_abort_on_non_zero_status': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_orte_app_num': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_orte_base_help_aggregate': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_orte_ess_node_rank': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_orte_ess_num_procs': '8',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_orte_hnp_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_orte_launch': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_orte_local_daemon_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_orte_num_nodes': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_orte_precondition_transports': '99cb0754aa46131b-b765a7ffda563ac1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_orte_tag_output': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_orte_tmpdir_base': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.algo-1.0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_plm_rsh_no_tree_spawn': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_pmix': '^s1,s2,cray,isolated',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_pml': 'ob1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_MCA_rmaps_base_display_map': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_rmaps_base_mapping_policy': 'slot',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_NUM_APP_CTX': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'OMPI_UNIVERSE_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'PATH': '/home/.openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds21_41',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds12_41',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PMIX_GDS_MODULE': 'ds21,ds12,hash',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PMIX_ID': '2697920513.0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'PMIX_MCA_mca_base_component_show_load_errors': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PMIX_NAMESPACE': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PMIX_PTL_MODULE': 'tcp,usock',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'PMIX_RANK': '0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PMIX_SECURITY_MODE': 'native',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'PMIX_SERVER_TMPDIR': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PMIX_SERVER_URI2': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'PMIX_SERVER_URI21': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'PMIX_SERVER_URI3': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PMIX_SYSTEM_TMPDIR': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PWD': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'PYTHONDONTWRITEBYTECODE': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'PYTHONIOENCODING': 'UTF-8',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'PYTHONUNBUFFERED': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SAGEMAKER_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SAGEMAKER_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SHLVL': '2',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_CHANNELS': '[\"train\"]',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_FRAMEWORK_PARAMS': '{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                        '--mca '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                        'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                        '0 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                        '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_HOSTS': '[\"algo-1\"]',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HPS': '{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843}',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_HP_APPLY_OPTIMIZER': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_BERT_MODEL': 'bert-large-uncased',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_CONFIG_FILE': 'bert_config.json',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_DO_TRAIN': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_INPUT_DIR': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_HP_LEARNING_RATE': '0.006',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_HP_MAX_PREDICTIONS_PER_SEQ': '20',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_MAX_SEQ_LENGTH': '128',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_MAX_STEPS': '7038',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_MP_PARAMETERS': '{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_NUM_STEPS_PER_CHECKPOINT': '200',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_OUTPUT_DIR': './checkpoints',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_HP_SEED': '12439',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_HP_SKIP_CHECKPOINT': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_SMP': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_HP_STEPS_THIS_RUN': '500',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_HP_TRAIN_BATCH_SIZE': '48',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_HP_USE_SEQUENTIAL': '1',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_HP_WARMUP_PROPORTION': '0.2843',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_INPUT_DATA_CONFIG': '{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_INPUT_DIR': '/opt/ml/input',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_LOG_LEVEL': '20',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_MODEL_DIR': '/opt/ml/model',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_MODULE_DIR': 's3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_MODULE_NAME': 'sagemaker_smp_pretrain',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_NETWORK_INTERFACE_NAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_NUM_CPUS': '32',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_NUM_GPUS': '4',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_OUTPUT_DIR': '/opt/ml/output',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                    '--mca '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                    'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                    '0 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                    '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2023-05-05-08-49-07-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_smp_pretrain\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_smp_pretrain.py\"}',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'SM_USER_ARGS': '[\"--allreduce_post_accumulation\",\"1\",\"--allreduce_post_accumulation_fp16\",\"1\",\"--apply_optimizer\",\"1\",\"--bert_model\",\"bert-large-uncased\",\"--config_file\",\"bert_config.json\",\"--do_train\",\"1\",\"--input_dir\",\"/opt/ml/input/data/train\",\"--learning_rate\",\"0.006\",\"--max_predictions_per_seq\",\"20\",\"--max_seq_length\",\"128\",\"--max_steps\",\"7038\",\"--mp_parameters\",\"ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster\",\"--num_steps_per_checkpoint\",\"200\",\"--output_dir\",\"./checkpoints\",\"--seed\",\"12439\",\"--skip_checkpoint\",\"1\",\"--smp\",\"1\",\"--steps_this_run\",\"500\",\"--train_batch_size\",\"48\",\"--use_sequential\",\"1\",\"--warmup_proportion\",\"0.2843\"]',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'SM_USER_ENTRY_POINT': 'sagemaker_smp_pretrain.py',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'TORCH_CUDA_ARCH_LIST': '3.5 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                         '3.7 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                         '5.2 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                         '6.0 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                         '6.1 '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                         [1,0]<stdout>:'7.0+PTX '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                         '8.0',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'TORCH_NVCC_FLAGS': '-Xfatbin '\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:                     '-compress-all',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:601636808299:training-job/pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 'TRAINING_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:'_': '/home/.openmpi/bin/mpirun.real'}\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:{'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-5e3c62bc095f77609317142ee3696e0d3eebe9bcb073bca17a438a61a9fb9a15-customer',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'AWS_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'CMAKE_PREFIX_PATH': '$(dirname '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                      '$(which '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                      'conda))/../',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'CUDA_VERSION': '11.0.3',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'CUDNN_VERSION': '8.0.4.30',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'DGLBACKEND': 'pytorch',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'DMLC_INTERFACE': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'HFI_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'HOME': '/root',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'HOROVOD_VERSION': '0.20.3',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'HOSTNAME': 'ip-10-0-167-116.us-east-2.compute.internal',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'IPATH_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'LANG': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'LC_ALL': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'LD_LIBRARY_PATH': '/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib:/home/.openmpi/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'LD_PRELOAD': '/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'MANUAL_BUILD': '0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'MASTER_ADDR': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'MASTER_PORT': '7777',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'NCCL_DEBUG': 'INFO',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'NCCL_IB_DISABLE': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'NCCL_MIN_NRINGS': '4',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'NCCL_SOCKET_IFNAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'NCCL_VERSION': '2.7.8',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.0 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                        'brand=tesla,driver>=418,driver<419 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                        [1,5]<stdout>:'brand=tesla,driver>=440,driver<441 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                        'brand=tesla,driver>=450,driver<451',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'NVIDIA_VISIBLE_DEVICES': 'all',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_APP_CTX_NUM_PROCS': '8',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_ARGV': '-m '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              'mpi4py '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              'sagemaker_smp_pretrain.py '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--allreduce_post_accumulation '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'--allreduce_post_accumulation_fp16 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--apply_optimizer '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'--bert_model '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              'bert-large-uncased '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--config_file '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              'bert_config.json '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'--do_train '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--input_dir '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'/opt/ml/input/data/train '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--learning_rate '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '0.006 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'--max_predictions_per_seq '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '20 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--max_seq_length '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '128 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'--max_steps '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '7038 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--mp_parameters '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              'ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'--num_steps_per_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '200 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--output_dir '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'./checkpoints '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--seed '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '12439 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--skip_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--smp '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'--steps_this_run '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '500 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--train_batch_size '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'48 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--use_sequential '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              '--warmup_proportion '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:              [1,5]<stdout>:'0.2843',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_COMMAND': 'python3.6',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_COMM_WORLD_LOCAL_RANK': '5',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_COMM_WORLD_LOCAL_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_COMM_WORLD_NODE_RANK': '5',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_COMM_WORLD_RANK': '5',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_COMM_WORLD_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_FILE_LOCATION': '/tmp/ompi.algo-1.0/pid.41/0/0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_FIRST_RANKS': '0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_MCA_btl': '^openib',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_btl_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_ess': '^singleton',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_MCA_ess_base_jobid': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_ess_base_vpid': '5',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_MCA_hwloc_base_binding_policy': 'none',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_initial_wdir': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_mpi_yield_when_idle': '0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_oob_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_MCA_orte_abort_on_non_zero_status': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_orte_app_num': '0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_MCA_orte_base_help_aggregate': '0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_orte_ess_node_rank': '5',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_MCA_orte_ess_num_procs': '8',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_orte_hnp_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_orte_launch': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_orte_local_daemon_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_MCA_orte_num_nodes': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_orte_precondition_transports': '99cb0754aa46131b-b765a7ffda563ac1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_MCA_orte_tag_output': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_orte_tmpdir_base': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.algo-1.0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_plm_rsh_no_tree_spawn': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_pmix': '^s1,s2,cray,isolated',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_MCA_pml': 'ob1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_rmaps_base_display_map': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_rmaps_base_mapping_policy': 'slot',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'OMPI_NUM_APP_CTX': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'OMPI_UNIVERSE_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'PATH': '/home/.openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds21_41',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds12_41',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'PMIX_GDS_MODULE': 'ds21,ds12,hash',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PMIX_ID': '2697920513.5',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PMIX_MCA_mca_base_component_show_load_errors': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PMIX_NAMESPACE': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'PMIX_PTL_MODULE': 'tcp,usock',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PMIX_RANK': '5',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'PMIX_SECURITY_MODE': 'native',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'PMIX_SERVER_TMPDIR': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PMIX_SERVER_URI2': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PMIX_SERVER_URI21': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'PMIX_SERVER_URI3': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PMIX_SYSTEM_TMPDIR': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'PWD': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PYTHONDONTWRITEBYTECODE': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PYTHONIOENCODING': 'UTF-8',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'PYTHONUNBUFFERED': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SAGEMAKER_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SAGEMAKER_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SHLVL': '2',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_CHANNELS': '[\"train\"]',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_FRAMEWORK_PARAMS': '{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                        '--mca '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                        'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                        '0 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                        '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HOSTS': '[\"algo-1\"]',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_HPS': '{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843}',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_APPLY_OPTIMIZER': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_BERT_MODEL': 'bert-large-uncased',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_HP_CONFIG_FILE': 'bert_config.json',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_DO_TRAIN': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_INPUT_DIR': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_HP_LEARNING_RATE': '0.006',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_MAX_PREDICTIONS_PER_SEQ': '20',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_HP_MAX_SEQ_LENGTH': '128',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_HP_MAX_STEPS': '7038',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_MP_PARAMETERS': '{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_HP_NUM_STEPS_PER_CHECKPOINT': '200',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_HP_OUTPUT_DIR': './checkpoints',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_SEED': '12439',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_SKIP_CHECKPOINT': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_HP_SMP': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_STEPS_THIS_RUN': '500',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_TRAIN_BATCH_SIZE': '48',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_HP_USE_SEQUENTIAL': '1',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_HP_WARMUP_PROPORTION': '0.2843',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_INPUT_DATA_CONFIG': '{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_INPUT_DIR': '/opt/ml/input',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_LOG_LEVEL': '20',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_MODEL_DIR': '/opt/ml/model',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_MODULE_DIR': 's3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_MODULE_NAME': 'sagemaker_smp_pretrain',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_NETWORK_INTERFACE_NAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_NUM_CPUS': '32',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_NUM_GPUS': '4',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_OUTPUT_DIR': '/opt/ml/output',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                    '--mca '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                    'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                    '0 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                    '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2023-05-05-08-49-07-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_smp_pretrain\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_smp_pretrain.py\"}',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'SM_USER_ARGS': '[\"--allreduce_post_accumulation\",\"1\",\"--allreduce_post_accumulation_fp16\",\"1\",\"--apply_optimizer\",\"1\",\"--bert_model\",\"bert-large-uncased\",\"--config_file\",\"bert_config.json\",\"--do_train\",\"1\",\"--input_dir\",\"/opt/ml/input/data/train\",\"--learning_rate\",\"0.006\",\"--max_predictions_per_seq\",\"20\",\"--max_seq_length\",\"128\",\"--max_steps\",\"7038\",\"--mp_parameters\",\"ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster\",\"--num_steps_per_checkpoint\",\"200\",\"--output_dir\",\"./checkpoints\",\"--seed\",\"12439\",\"--skip_checkpoint\",\"1\",\"--smp\",\"1\",\"--steps_this_run\",\"500\",\"--train_batch_size\",\"48\",\"--use_sequential\",\"1\",\"--warmup_proportion\",\"0.2843\"]',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'SM_USER_ENTRY_POINT': 'sagemaker_smp_pretrain.py',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'TORCH_CUDA_ARCH_LIST': '3.5 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                         '3.7 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                         '5.2 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                         '6.0 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                         '6.1 '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                         '7.0+PTX '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                         [1,5]<stdout>:'8.0',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: 'TORCH_NVCC_FLAGS': '-Xfatbin '\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:                     '-compress-all',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:601636808299:training-job/pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: [1,5]<stdout>:'TRAINING_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: '_': '/home/.openmpi/bin/mpirun.real'}\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:{'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-5e3c62bc095f77609317142ee3696e0d3eebe9bcb073bca17a438a61a9fb9a15-customer',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'AWS_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'CMAKE_PREFIX_PATH': '$(dirname '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                      '$(which '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                      'conda))/../',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'CUDA_VERSION': '11.0.3',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'CUDNN_VERSION': '8.0.4.30',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'DGLBACKEND': 'pytorch',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'DMLC_INTERFACE': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'HFI_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'HOME': '/root',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'HOROVOD_VERSION': '0.20.3',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'HOSTNAME': 'ip-10-0-167-116.us-east-2.compute.internal',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'IPATH_NO_BACKTRACE': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'LANG': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'LC_ALL': 'C.UTF-8',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'LD_LIBRARY_PATH': '/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib:/home/.openmpi/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'LD_PRELOAD': '/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'MANUAL_BUILD': '0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'MASTER_ADDR': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'MASTER_PORT': '7777',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'NCCL_DEBUG': 'INFO',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'NCCL_IB_DISABLE': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'NCCL_MIN_NRINGS': '4',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'NCCL_SOCKET_IFNAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'NCCL_VERSION': '2.7.8',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.0 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                        'brand=tesla,driver>=418,driver<419 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                        'brand=tesla,driver>=440,driver<441 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                        [1,3]<stdout>:'brand=tesla,driver>=450,driver<451',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'NVIDIA_VISIBLE_DEVICES': 'all',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_APP_CTX_NUM_PROCS': '8',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_ARGV': '-m '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              'mpi4py '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              'sagemaker_smp_pretrain.py '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--allreduce_post_accumulation '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'--allreduce_post_accumulation_fp16 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--apply_optimizer '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--bert_model '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              'bert-large-uncased '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'--config_file '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              'bert_config.json '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--do_train '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'--input_dir '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '/opt/ml/input/data/train '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--learning_rate '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '0.006 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'--max_predictions_per_seq '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '20 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'--max_seq_length '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '128 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--max_steps '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '7038 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'--mp_parameters '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              'ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'--num_steps_per_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '200 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--output_dir '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'./checkpoints '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--seed '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'12439 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--skip_checkpoint '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--smp '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'1 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--steps_this_run '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '500 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'--train_batch_size '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '48 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '--use_sequential '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '1 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              [1,3]<stdout>:'--warmup_proportion '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:              '0.2843',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_COMMAND': 'python3.6',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_COMM_WORLD_LOCAL_RANK': '3',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_COMM_WORLD_LOCAL_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_COMM_WORLD_NODE_RANK': '3',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_COMM_WORLD_RANK': '3',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_COMM_WORLD_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_FILE_LOCATION': '/tmp/ompi.algo-1.0/pid.41/0/0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_FIRST_RANKS': '0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_btl': '^openib',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_btl_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_btl_vader_single_copy_mechanism': 'none',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_ess': '^singleton',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_ess_base_jobid': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_ess_base_vpid': '3',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_hwloc_base_binding_policy': 'none',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_initial_wdir': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_mpi_yield_when_idle': '0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_oob_tcp_if_include': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_orte_abort_on_non_zero_status': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_orte_app_num': '0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_orte_base_help_aggregate': '0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_orte_ess_node_rank': '3',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_orte_ess_num_procs': '8',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_orte_hnp_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_orte_launch': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_orte_local_daemon_uri': '2697920512.0;tcp://10.0.167.116:34877',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_orte_num_nodes': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_orte_precondition_transports': '99cb0754aa46131b-b765a7ffda563ac1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_orte_tag_output': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_orte_tmpdir_base': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.algo-1.0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_plm_rsh_no_tree_spawn': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_pmix': '^s1,s2,cray,isolated',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_pml': 'ob1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_MCA_rmaps_base_display_map': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_rmaps_base_mapping_policy': 'slot',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'OMPI_NUM_APP_CTX': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'OMPI_UNIVERSE_SIZE': '8',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PATH': '/home/.openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds21_41',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.algo-1.0/pid.41/pmix_dstor_ds12_41',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'PMIX_GDS_MODULE': 'ds21,ds12,hash',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PMIX_ID': '2697920513.3',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PMIX_MCA_mca_base_component_show_load_errors': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'PMIX_NAMESPACE': '2697920513',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'PMIX_PTL_MODULE': 'tcp,usock',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PMIX_RANK': '3',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PMIX_SECURITY_MODE': 'native',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PMIX_SERVER_TMPDIR': '/tmp/ompi.algo-1.0/pid.41',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'PMIX_SERVER_URI2': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PMIX_SERVER_URI21': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PMIX_SERVER_URI3': '2697920512.0;tcp4://127.0.0.1:58881',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PMIX_SYSTEM_TMPDIR': '/tmp',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'PWD': '/opt/ml/code',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PYTHONDONTWRITEBYTECODE': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'PYTHONIOENCODING': 'UTF-8',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'PYTHONUNBUFFERED': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SAGEMAKER_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SAGEMAKER_REGION': 'us-east-2',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SHLVL': '2',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_CHANNELS': '[\"train\"]',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_CURRENT_HOST': 'algo-1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_FRAMEWORK_PARAMS': '{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                        '--mca '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                        'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                        '0 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                        '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_HOSTS': '[\"algo-1\"]',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HPS': '{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843}',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_HP_ALLREDUCE_POST_ACCUMULATION': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_APPLY_OPTIMIZER': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_HP_BERT_MODEL': 'bert-large-uncased',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_CONFIG_FILE': 'bert_config.json',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_HP_DO_TRAIN': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_INPUT_DIR': '/opt/ml/input/data/train',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_HP_LEARNING_RATE': '0.006',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_MAX_PREDICTIONS_PER_SEQ': '20',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_HP_MAX_SEQ_LENGTH': '128',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_MAX_STEPS': '7038',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_MP_PARAMETERS': '{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_HP_NUM_STEPS_PER_CHECKPOINT': '200',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_OUTPUT_DIR': './checkpoints',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_HP_SEED': '12439',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_SKIP_CHECKPOINT': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_SMP': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_STEPS_THIS_RUN': '500',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_HP_TRAIN_BATCH_SIZE': '48',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_USE_SEQUENTIAL': '1',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_HP_WARMUP_PROPORTION': '0.2843',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_INPUT_DATA_CONFIG': '{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_INPUT_DIR': '/opt/ml/input',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_LOG_LEVEL': '20',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_MODEL_DIR': '/opt/ml/model',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_MODULE_DIR': 's3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_MODULE_NAME': 'sagemaker_smp_pretrain',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_NETWORK_INTERFACE_NAME': 'eth0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_NUM_CPUS': '32',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_NUM_GPUS': '4',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_OUTPUT_DIR': '/opt/ml/output',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                    '--mca '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                    'orte_base_help_aggregate '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                    '0 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                    '\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2023-05-05-08-49-07-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-601636808299/pytorch-training-2023-05-05-08-49-07-014/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_smp_pretrain\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_smp_pretrain.py\"}',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'SM_USER_ARGS': '[\"--allreduce_post_accumulation\",\"1\",\"--allreduce_post_accumulation_fp16\",\"1\",\"--apply_optimizer\",\"1\",\"--bert_model\",\"bert-large-uncased\",\"--config_file\",\"bert_config.json\",\"--do_train\",\"1\",\"--input_dir\",\"/opt/ml/input/data/train\",\"--learning_rate\",\"0.006\",\"--max_predictions_per_seq\",\"20\",\"--max_seq_length\",\"128\",\"--max_steps\",\"7038\",\"--mp_parameters\",\"ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster\",\"--num_steps_per_checkpoint\",\"200\",\"--output_dir\",\"./checkpoints\",\"--seed\",\"12439\",\"--skip_checkpoint\",\"1\",\"--smp\",\"1\",\"--steps_this_run\",\"500\",\"--train_batch_size\",\"48\",\"--use_sequential\",\"1\",\"--warmup_proportion\",\"0.2843\"]',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'SM_USER_ENTRY_POINT': 'sagemaker_smp_pretrain.py',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'TORCH_CUDA_ARCH_LIST': '3.5 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                         '3.7 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                         '5.2 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                         '6.0 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                         '6.1 '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                         [1,3]<stdout>:'7.0+PTX '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                         '8.0',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'TORCH_NVCC_FLAGS': '-Xfatbin '\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:                     '-compress-all',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:601636808299:training-job/pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: 'TRAINING_JOB_NAME': 'pytorch-training-2023-05-05-08-49-07-014',\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: [1,3]<stdout>:'_': '/home/.openmpi/bin/mpirun.real'}\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m2023-05-05 08:54:10,248 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose --mca orte_base_help_aggregate 0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAIN -x SM_HP_ALLREDUCE_POST_ACCUMULATION -x SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16 -x SM_HP_APPLY_OPTIMIZER -x SM_HP_BERT_MODEL -x SM_HP_CONFIG_FILE -x SM_HP_DO_TRAIN -x SM_HP_INPUT_DIR -x SM_HP_LEARNING_RATE -x SM_HP_MAX_PREDICTIONS_PER_SEQ -x SM_HP_MAX_SEQ_LENGTH -x SM_HP_MAX_STEPS -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_STEPS_PER_CHECKPOINT -x SM_HP_OUTPUT_DIR -x SM_HP_SEED -x SM_HP_SKIP_CHECKPOINT -x SM_HP_SMP -x SM_HP_STEPS_THIS_RUN -x SM_HP_TRAIN_BATCH_SIZE -x SM_HP_USE_SEQUENTIAL -x SM_HP_WARMUP_PROPORTION -x PYTHONPATH /opt/conda/bin/python3.6 -m mpi4py sagemaker_smp_pretrain.py --allreduce_post_accumulation 1 --allreduce_post_accumulation_fp16 1 --apply_optimizer 1 --bert_model bert-large-uncased --config_file bert_config.json --do_train 1 --input_dir /opt/ml/input/data/train --learning_rate 0.006 --max_predictions_per_seq 20 --max_seq_length 128 --max_steps 7038 --mp_parameters ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster --num_steps_per_checkpoint 200 --output_dir ./checkpoints --seed 12439 --skip_checkpoint 1 --smp 1 --steps_this_run 500 --train_batch_size 48 --use_sequential 1 --warmup_proportion 0.2843\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirments.txt'\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 1093, in <module>\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    args, final_loss, train_time_raw, global_step = main()\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 804, in main\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    device, args = setup_training(args)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 434, in setup_training\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    smp.init()\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/torch/__init__.py\", line 92, in init\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    fork=False,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/__init__.py\", line 29, in init_internal\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, fork=fork\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 53, in maybe_fork_and_initialize\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    _initialize(path, cfg, core, state, true_if_pt_else_tf, start_pipeline_threads, child=False)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/fork.py\", line 64, in _initialize\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34mthe job to be terminated. The first process to do so was:\n",
      "  Process name: [[41167,1],4]\n",
      "  Exit code:    1\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    core.initialize(path, cfg, true_if_pt_else_tf, start_pipeline_threads)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/smdistributed/modelparallel/backend/core.py\", line 87, in initialize\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    f\"The number of node-local MPI processes {self.lib.smp_local_size()}\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:RuntimeError: The number of node-local MPI processes 8cannot be larger than the device count 4\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[34ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[34mthe job to be terminated. The first process to do so was:\n",
      "  Process name: [[41167,1],4]\n",
      "  Exit code:    1\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "2023-05-05 08:54:22 Uploading - Uploading generated training model\n",
      "2023-05-05 08:54:22 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2023-05-05-08-49-07-014: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose --mca orte_base_help_aggregate 0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpytorch_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:284\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:1198\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:2344\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2342\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:4628\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4608\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4609\u001b[0m \n\u001b[1;32m   4610\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4626\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4627\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4628\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:6490\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6487\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 6490\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   6492\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:6543\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   6538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   6539\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6540\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6541\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6542\u001b[0m     )\n\u001b[0;32m-> 6543\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6544\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6545\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6546\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6547\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2023-05-05-08-49-07-014: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose --mca orte_base_help_aggregate 0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x "
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit(data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/training|distributed_training|pytorch|model_parallel|bert|smp_bert_tutorial.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
